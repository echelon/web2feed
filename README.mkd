web2feed: turn webpages into feeds
==================================
This is a script to turn any webpage into a feed. The program relies first on
site-specific rules (for popular sites and detectable software packages), then
on heuristics (_TODO_).

The premise is that _RSS/Atom feeds often don't tell the whole story_. We
should be able to scrape any webpage for content regardless of what the
author wants to make easily available.

Output is a list of dictionaries, but serializations (JSON, XML, RDF) will be
supported. Additional options to be developed include advertisement/javascript
removal, link/image/media isolation, etc.

Ultimately, this was written as a support package for 
[Sylph](http://github.com/echelon/sylph), which aims to completely decentralize
the web and take bootstrapped content with it. _(Please read more about that 
and consider contributing.)_

The following libraries are used:

* BeautifulSoup
* html5lib (for beautiful soup parser fixes)
* simplejson

(A complete client would have no non-standard library dependencies.)

Sites/Blogs
-----

* [Slashdot](http://slashdot.org)
* [Reddit](http://reddit.com)
* [Techcrunch](http://techcrunch.com)
* [Techdirt](http://techdirt.com)
* [Less Wrong](http://lesswrong.com)
* [Biology News](http://www.biologynews.net)
* [Marc Canter's blog](http://blog.broadbandmechanics.com)
* [Robert Scoble's blog](http://scobleizer.com)
* [Aaron Swartz' blog](http://www.aaronsw.com/weblog/)

Software
--------

**None yet**

(Also, heuristic scraping hasn't even been started yet.)

Output format:
--------------

Output is a list of dictionaries, with the following keys:

* **uri**
* **title**
* **date** (a python datetime object, but may only include date component)
* **author**
* **contents** and/or **summary**, which probably contain minor HTML 
  such as <p> and <img>

More will be added as I write code to support comments, etc. Also to be output is the type of page, the heuristics used, etc.

License
-------
Licensed under the BSD.


